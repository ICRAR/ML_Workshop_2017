{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Galaxy Zoo with keras\n",
    "\n",
    "\n",
    "Scroll down for the main keras code. The top section is for creating validation and sample data.\n",
    "\n",
    "## Create Train/Test/Validation/Sample data\n",
    "\n",
    "You can use the below code to help move files from the training folder to the validation folder, and to create a small amount of sample data for debugging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "\n",
    "# g = glob.glob(data_path+'train/*.jpg')\n",
    "# shuf = np.random.permutation(g)\n",
    "# for i in range(2000):\n",
    "#     os.rename(shuf[i], data_path+ 'valid/' + shuf[i].split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy files into sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# g = glob.glob(data_path+\"train/*.jpg\") # find all path names matching this regex pattern\n",
    "# shuf = np.random.permutation(g)\n",
    "# for i in range(2000):\n",
    "#     copyfile(shuf[i], data_path+\"sample/train/\" + shuf[i].split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# valid_path = data_path + 'valid/'\n",
    "# g = glob.glob(valid_path + '*.jpg') # find all path names matching this regex pattern\n",
    "# shuf = np.random.permutation(g)\n",
    "# for i in range(200):\n",
    "#     copyfile(shuf[i], data_path+\"sample/valid/\" + shuf[i].split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_path = data_path + 'test/'\n",
    "# g = glob.glob(test_path + '*.jpg') # find all path names matching this regex pattern\n",
    "# shuf = np.random.permutation(g)\n",
    "# for i in range(2000):\n",
    "#     copyfile(shuf[i], data_path+\"sample/test/\" + shuf[i].split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda, Reshape\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "#image_dim_ordering:\"th\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/keegan/43379bb7-2565-4466-8a93-aa969ea76c5d/keegan/Anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  import sys\n",
      "/media/keegan/43379bb7-2565-4466-8a93-aa969ea76c5d/keegan/Anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D((2, 2), strides=(2, 2), data_format=\"channels_first\")`\n",
      "  \n",
      "/media/keegan/43379bb7-2565-4466-8a93-aa969ea76c5d/keegan/Anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
      "  import sys\n",
      "/media/keegan/43379bb7-2565-4466-8a93-aa969ea76c5d/keegan/Anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
      "  import sys\n",
      "/media/keegan/43379bb7-2565-4466-8a93-aa969ea76c5d/keegan/Anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "def ConvBlock(layers, model, filters):\n",
    "    \"\"\"\n",
    "    Create a layered Conv/Pooling block\n",
    "    \"\"\"\n",
    "    for i in range(layers): \n",
    "        model.add(ZeroPadding2D((1,1)))  # zero padding of size 1\n",
    "        model.add(Conv2D(filters, 3, 3, activation='relu'))  # 3x3 filter size \n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2), dim_ordering=\"th\"))\n",
    "\n",
    "def FCBlock(model):\n",
    "    \"\"\"\n",
    "    Fully connected block with ReLU and dropout\n",
    "    \"\"\"\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "def VGG_16():\n",
    "    \"\"\"\n",
    "    Implement VGG16 architecture\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x : x, input_shape=(3,106,106)))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2), dim_ordering=\"th\"))\n",
    "    \n",
    "    ConvBlock(2, model, 64)\n",
    "    ConvBlock(2, model, 128)\n",
    "    ConvBlock(3, model, 256)\n",
    "    ConvBlock(3, model, 512)\n",
    "    ConvBlock(3, model, 512)\n",
    "\n",
    "    model.add(Flatten())\n",
    "    FCBlock(model)\n",
    "    FCBlock(model)\n",
    "    \n",
    "    model.add(Dense(37, activation = 'sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Compile \n",
    "optimizer = RMSprop(lr=1e-6)\n",
    "model = VGG_16()\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/images/training/train\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "from scipy.misc import imresize  \n",
    "\n",
    "class data_getter:    \n",
    "    \"\"\"\n",
    "    Creates a class for handling train/valid/test data paths,\n",
    "    training labels and image IDs.\n",
    "    Useful for switching between sample and full datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, path):    \n",
    "        self.path = path \n",
    "        self.train_path = path + \"train\"\n",
    "        self.val_path = path + \"valid\"\n",
    "        self.test_path = path + \"test\"\n",
    "        \n",
    "        def get_paths(directory):\n",
    "            return [f for f in os.listdir(directory)]\n",
    "        \n",
    "        self.training_images_paths = get_paths(self.train_path)\n",
    "        self.validation_images_paths = get_paths(self.val_path)\n",
    "        self.test_images_paths = get_paths(self.test_path)    \n",
    "        \n",
    "        def get_all_solutions():\n",
    "        ### Import solutions file and load into self.solutions\n",
    "            import csv\n",
    "            all_solutions = {}\n",
    "            with open('training_solutions_rev1.csv', 'r') as f:\n",
    "                reader = csv.reader(f, delimiter=\",\")\n",
    "                next(reader)\n",
    "                for i, line in enumerate(reader):\n",
    "                    all_solutions[line[0]] = [float(x) for x in line[1:]]\n",
    "            return all_solutions\n",
    "        \n",
    "        self.all_solutions = get_all_solutions()\n",
    "\n",
    "    def get_id(self,fname):\n",
    "        return fname.replace(\".jpg\",\"\").replace(\"data\",\"\")\n",
    "        \n",
    "    def find_label(self,val):\n",
    "        return self.all_solutions[val]\n",
    "        \n",
    "# fetcher = data_getter('data/sample/')\n",
    "fetcher = data_getter('data/images/training/')\n",
    "print(fetcher.train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_images(data/):\n",
    "    \"\"\"\n",
    "    Import image at 'paths', decode, centre crop and prepare for batching. \n",
    "    \"\"\"\n",
    "    count = len(paths)\n",
    "    arr = np.zeros(shape=(count,3,106,106))\n",
    "    for c, path in enumerate(paths):\n",
    "        img = plt.imread(path).T\n",
    "        img = img[:,106:106*3,106:106*3] #crop 424x424 -> 212x212\n",
    "        img = imresize(img,size=(106,106,3),interp=\"cubic\").T # downsample to half res\n",
    "        arr[c] = img\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print some before/after processing images\n",
    "\n",
    "#process_images([fetcher.train_path + '/' + fetcher.training_images_paths[100]])#\n",
    "#im = plt.imread(fetcher.train_path + '/' + fetcher.training_images_paths[0])\n",
    "#print(im.shape)#\n",
    "\n",
    "#plt.imshow(im)\n",
    "#plt.show()\n",
    "#im = im.T[:,106:106*3,106:106*3] #crop 424x424 -> 212x212\n",
    "#im = imresize(im,size=(106,106,3),interp=\"cubic\").T # downsample to half res#\n",
    "#print(im.shape)#\n",
    "#plt.imshow(im.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create generator that yields (current features X, current labels y)\n",
    "def BatchGenerator(getter):\n",
    "    while 1:\n",
    "        for f in getter.training_images_paths:\n",
    "            X_train = process_images([getter.train_path + '/' + fname for fname in [f]])\n",
    "            id_ = getter.get_id(f)\n",
    "            y_train = np.array(getter.find_label(id_))\n",
    "            y_train = np.reshape(y_train,(1,37))\n",
    "            yield (X_train, y_train)\n",
    "            \n",
    "def ValBatchGenerator(getter):\n",
    "    while 1:\n",
    "        for f in getter.validation_images_paths:\n",
    "            X_train = process_images([getter.val_path + '/' + fname for fname in [f]])\n",
    "            id_ = getter.get_id(f)\n",
    "            y_train = np.array(getter.find_label(id_))\n",
    "            y_train = np.reshape(y_train,(1,37))\n",
    "            yield (X_train, y_train)\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "Unable to open file (Unable to open file: name = 'data/weights.hdf5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-256-12d9ddfde409>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/weights.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \"\"\" \n\u001b[1;32m     30\u001b[0m hist = model.fit_generator(BatchGenerator(fetcher),\n",
      "\u001b[0;32m/media/keegan/43379bb7-2565-4466-8a93-aa969ea76c5d/keegan/Anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m         \u001b[0;31m# instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/keegan/43379bb7-2565-4466-8a93-aa969ea76c5d/keegan/Anaconda2/lib/python2.7/site-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/keegan/43379bb7-2565-4466-8a93-aa969ea76c5d/keegan/Anaconda2/lib/python2.7/site-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: Unable to open file (Unable to open file: name = 'data/weights.hdf5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "    \n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7, verbose=1, mode='auto')\n",
    "history = LossHistory()\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath='data/weights.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "steps_to_take = int(len(fetcher.training_images_paths)/batch_size)\n",
    "val_steps_to_take = int(len(fetcher.validation_images_paths)/batch_size)\n",
    "                #typically be equal to the number of unique samples if your dataset\n",
    "                #divided by the batch size.\n",
    "        \n",
    "       \n",
    "model = load_model('data/weights.hdf5')\n",
    "\"\"\" \n",
    "hist = model.fit_generator(BatchGenerator(fetcher),\n",
    "                    samples_per_epoch=steps_to_take, \n",
    "                    nb_epoch=50,\n",
    "                    validation_data=ValBatchGenerator(fetcher),\n",
    "                    nb_val_samples=val_steps_to_take,\n",
    "                    verbose=2,\n",
    "                    callbacks=[history,checkpointer,early_stopping],\n",
    "                   )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training/validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(12,8))\n",
    "#plt.plot(hist.epoch,hist.history['loss'],label='Test')\n",
    "#plt.plot(hist.epoch,hist.history['val_loss'],label='Validation',linestyle='--')\n",
    "#plt.xlabel(\"Epochs\")\n",
    "#plt.ylabel(\"RMSE\")\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model weights\n",
    "from keras.models import load_model\n",
    "model = load_model('tmp/weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TestBatchGenerator(getter):\n",
    "    while 1:\n",
    "        for f in getter.test_images_paths:\n",
    "            X_train = process_images([getter.test_path + '/' + fname for fname in [f]])\n",
    "            yield (X_train)\n",
    "\n",
    "predictions = model.predict_generator(TestBatchGenerator(fetcher),\n",
    "                       val_samples = len(fetcher.test_images_paths),\n",
    "                        max_q_size = 32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions.shape\n",
    "print \"Working\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = open('all_zeros_benchmark.csv','r').readlines()[0]\n",
    "\n",
    "with open('submission_1.csv','w') as outfile:\n",
    "    outfile.write(header)\n",
    "    for i in range(len(fetcher.test_images_paths)):\n",
    "        id_ = (fetcher.get_id(fetcher.test_images_paths[i]))\n",
    "        pred = predictions[i]\n",
    "        outline = id_ + \",\" + \",\".join([str(x) for x in pred])\n",
    "        outfile.write(outline + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
