{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Process\n",
    "We are able to evaluate the model against test dataset both after and in parallel with the training process. We aim to perform the latter method in this workshop. In the former, the evaluation performs on all the pre-build check-points however the latter evaluates every single checkpoint that the training process generates. Anyhow, let's go through the evaluation process.\n",
    "\n",
    "Again, we import `tensorflow`, `mnist`, `lenet`, and `load_batch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from datasets import mnist\n",
    "from model import lenet, load_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the train code, we shorten some directions and specify the flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim\n",
    "metrics = tf.contrib.metrics\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string('data_dir', './data/',\n",
    "                    'Directory with the MNIST data.')\n",
    "flags.DEFINE_integer('batch_size', 5, 'Batch size.')\n",
    "flags.DEFINE_integer('eval_interval_secs', 60,\n",
    "                    'Number of seconds between evaluations.')\n",
    "flags.DEFINE_integer('num_evals', 100, 'Number of batches to evaluate.')\n",
    "flags.DEFINE_string('log_dir', './log/eval/',\n",
    "                    'Directory where to log evaluation data.')\n",
    "flags.DEFINE_string('checkpoint_dir', './log/train/',\n",
    "                    'Directory with the model checkpoint data.')\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset using `mnist.get_split`. Notice that we load the test dataset here since we have to evaluate the model using a separate dataset from the training dataset. Otherwise, the accuracy will turn out an unrealistic value, i.e. 1 or so close. To test the quality of the recognition in real-world conditions, we must use digits that the system has NOT seen during training. Otherwise, it could learn all the training digits by heart and still fail at recognizing an \"8\" that I just wrote. The MNIST dataset contains 10,000 test digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = mnist.get_split('...', FLAGS.data_dir)\n",
    "\n",
    "images, labels = load_batch(\n",
    "    dataset,\n",
    "    FLAGS.batch_size,\n",
    "    is_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model prediction from the LeNet network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert prediction values for each class into single class prediction which is the highest probability for that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = tf.to_int64(tf.argmax(predictions, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is simply the % of correctly recognized digits. This is computed on the test set. You will see the values go up if the training goes well. 'streaming_accuracy' calculates how often predictions matches labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics_to_values, metrics_to_updates = metrics.aggregate_metric_map({\n",
    "    'mse': metrics.streaming_mean_squared_error(predictions, labels),\n",
    "    'accuracy': metrics.streaming_accuracy(predictions, labels),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the metrics values as summaries to be plotted later. We will be plotting the online evolution of accuracy on trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for metric_name, metric_value in metrics_to_values.iteritems():\n",
    "    tf.summary.scalar(metric_name, metric_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the instruction above, we are ready to launch the model evaluation. So, utilizing function `slim.evaluation.evaluation_loop` the checkpoints in the `checkpoint_dir` will run in a loop of evaluation with the time intervals of `eval_interval_secs`. Recall that we have specified the interval to be 60 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "...(\n",
    "    '',\n",
    "    FLAGS.checkpoint_dir,\n",
    "    FLAGS.log_dir,\n",
    "    num_evals=FLAGS.num_evals,\n",
    "    eval_op=metrics_to_updates.values(),\n",
    "    eval_interval_secs=FLAGS.eval_interval_secs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
