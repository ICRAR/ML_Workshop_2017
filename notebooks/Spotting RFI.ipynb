{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Spotting RFI in GMRT data\n",
    "\n",
    "This tutorial will show you how to use GMRT simulated data to spot RFI from \"normal\" noise.\n",
    "\n",
    "Firstly, I must thank Kaushal Buch from NCRA in India for letting me share the data with you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some standard imports and constants done first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from os import makedirs\n",
    "from os.path import exists\n",
    "import sys\n",
    "from timeit import default_timer\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from astropy.utils.console import human_time\n",
    "from statsmodels.robust import scale\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "H5_VERSION = '2017_10_20_001'\n",
    "NUMBER_CHANNELS = 1\n",
    "NUMBER_OF_CLASSES = 2\n",
    "URL_ROOT = 'http://ict.icrar.org/store/staff/kevin/rfi'\n",
    "HIDDEN_LAYERS = 200\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in a series of files stored on the ICRAR \"shared\" area. The files come as a pair <filename>.txt contains the voltage at the antenna, whilst <filename>_loc.txt contains 0 or 1 for the corresponding row to show whether the voltage is no RFI or RFI.\n",
    "\n",
    "Reading the text files is fastest in Pandas, but storing the data as Numpy arrays is fastest in HDF5 (h5py).\n",
    "\n",
    "We have four files representing different noise conditions.\n",
    "\n",
    "The HDF5 file stores the data as a series of points, with two classes. It is RFI or it isn't RFI. Once the HDF5 file has been "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class H5Exception(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class RfiData(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self._sequence_length = kwargs['sequence_length']\n",
    "        self._num_processes = kwargs['num_processes']\n",
    "        self._using_gpu = kwargs['using_gpu']\n",
    "        output_file = os.path.join(kwargs['data_path'], kwargs['data_file'])\n",
    "        with h5py.File(output_file, 'r') as h5_file:\n",
    "            data_group = h5_file['data']\n",
    "\n",
    "            # Move the data into memory\n",
    "            self._data_channel_0 = np.copy(data_group['data_channel_0'])\n",
    "            self._labels = np.copy(data_group['labels'])\n",
    "\n",
    "            length_data = len(self._labels) - kwargs['sequence_length']\n",
    "            split_point1 = int(length_data * kwargs['training_percentage'] / 100.)\n",
    "            split_point2 = int(length_data * (kwargs['training_percentage'] + kwargs['validation_percentage']) / 100.)\n",
    "            perm0 = np.arange(length_data)\n",
    "            np.random.shuffle(perm0)\n",
    "\n",
    "            self._train_sequence = perm0[:split_point1]\n",
    "            self._validation_sequence = perm0[split_point1:split_point2]\n",
    "            self._test_sequence = perm0[split_point2:]\n",
    "\n",
    "    def get_rfi_dataset(self, data_type, rank=None, short_run_size=None):\n",
    "        if data_type not in ['training', 'validation', 'test']:\n",
    "            raise ValueError(\"data_type must be one of: 'training', 'validation', 'test'\")\n",
    "\n",
    "        if data_type == 'training':\n",
    "            sequence = self._train_sequence\n",
    "        elif data_type == 'validation':\n",
    "            sequence = self._validation_sequence\n",
    "        else:\n",
    "            sequence = self._test_sequence\n",
    "\n",
    "        if self._using_gpu or rank is None:\n",
    "            if short_run_size is not None:\n",
    "                sequence = sequence[0:short_run_size]\n",
    "        else:\n",
    "            section_length = len(sequence) / self._num_processes\n",
    "            start = rank * section_length\n",
    "            if rank == self._num_processes - 1:\n",
    "                if short_run_size is not None:\n",
    "                    sequence = sequence[start:start + short_run_size]\n",
    "                else:\n",
    "                    sequence = sequence[start:]\n",
    "            else:\n",
    "                if short_run_size is not None:\n",
    "                    sequence = sequence[start:start + short_run_size]\n",
    "                else:\n",
    "                    sequence = sequence[start:start + section_length]\n",
    "\n",
    "        return RfiDataset(sequence, self._data_channel_0, self._labels, self._sequence_length)\n",
    "\n",
    "\n",
    "class RfiDataset(Dataset):\n",
    "    def __init__(self, selection_order, x_data, y_data, sequence_length):\n",
    "        self._x_data = x_data\n",
    "        self._y_data = y_data\n",
    "        self._selection_order = selection_order\n",
    "        self._length = len(selection_order)\n",
    "        self._sequence_length = sequence_length\n",
    "        self._actual_node = self._sequence_length / 2\n",
    "        self._median = np.median(x_data)\n",
    "        self._median_absolute_deviation = scale.mad(x_data, c=1)\n",
    "        self._mean = np.mean(x_data)\n",
    "        LOGGER.debug('Length: {}'.format(self._length))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        selection_index = self._selection_order[index]\n",
    "        x_data = self._x_data[selection_index:selection_index + self._sequence_length]\n",
    "        local_median = np.median(x_data)\n",
    "        local_median_absolute_deviation = scale.mad(x_data, c=1)\n",
    "        local_mean = np.mean(x_data)\n",
    "        # x_data_last = x_data[self._actual_node]\n",
    "\n",
    "        data = [self._median, self._median_absolute_deviation, self._mean, local_median, local_median_absolute_deviation, local_mean]\n",
    "        for item in x_data:\n",
    "            data.append(item)\n",
    "            data.append(item - self._mean)\n",
    "            data.append(item - self._median)\n",
    "            data.append(item - self._median_absolute_deviation)\n",
    "            data.append(item - local_mean)\n",
    "            data.append(item - local_median)\n",
    "            data.append(item - local_median_absolute_deviation)\n",
    "\n",
    "        # return np.reshape(x_data, (NUMBER_CHANNELS, -1)), values, self._y_data[selection_index + self._actual_node]\n",
    "        return np.array(data), self._y_data[selection_index + self._actual_node]\n",
    "\n",
    "\n",
    "def process_files(filename, rfi_label):\n",
    "    \"\"\" Process a file and return the data and the labels \"\"\"\n",
    "    files_to_process = []\n",
    "    for ending in ['.txt', '_loc.txt']:\n",
    "        complete_filename = filename + ending\n",
    "        files_to_process.append(complete_filename)\n",
    "\n",
    "    # Load the files into numpy\n",
    "    LOGGER.info('Loading: {}'.format(files_to_process[0]))\n",
    "    data_frame = pd.read_csv(files_to_process[0], header=None, delimiter=' ')\n",
    "    data = data_frame.values.flatten()\n",
    "\n",
    "    LOGGER.info('Loading: {}'.format(files_to_process[1]))\n",
    "    data_frame = pd.read_csv(files_to_process[1], header=None, delimiter=' ')\n",
    "    labels = data_frame.values.flatten()\n",
    "\n",
    "    # Check the lengths match\n",
    "    assert len(data) == len(labels), 'The line counts do not match for: {0}'.format(filename)\n",
    "\n",
    "    # If substitute of the label is needed\n",
    "    if rfi_label != 1:\n",
    "        labels[labels == 1] = rfi_label\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def build_data(**kwargs):\n",
    "    \"\"\" Read data \"\"\"\n",
    "    output_file = os.path.join(kwargs['data_path'], kwargs['data_file'])\n",
    "    if os.path.exists(output_file):\n",
    "        with h5py.File(output_file, 'r') as h5_file:\n",
    "            # Everything matches\n",
    "            if 'version' in h5_file.attrs and h5_file.attrs['version'] == H5_VERSION:\n",
    "                # All good nothing to do\n",
    "                return\n",
    "\n",
    "    # Open the output files\n",
    "    with Timer('Processing input files'):\n",
    "        data1, labels1 = process_files(URL_ROOT + '/impulsive_broadband_simulation_random_5p', 1)\n",
    "        data2, labels2 = process_files(URL_ROOT + '/impulsive_broadband_simulation_random_10p', 1)\n",
    "        data3, labels3 = process_files(URL_ROOT + '/repetitive_rfi_timeseries', 1)\n",
    "        data4, labels4 = process_files(URL_ROOT + '/repetitive_rfi_random_timeseries', 1)\n",
    "        # data0, labels0 = process_files(URL_ROOT + '/impulsive_broadband_simulation_random_norfi', 0)\n",
    "\n",
    "    # Concatenate\n",
    "    with Timer('Concatenating data'):\n",
    "        labels = np.concatenate((labels1, labels2, labels3, labels4))\n",
    "        data = np.concatenate((data1, data2, data3, data4))\n",
    "\n",
    "    # Standardise and one hot\n",
    "    with Timer('Standardise & One hot'):\n",
    "        labels = one_hot(labels, NUMBER_OF_CLASSES)\n",
    "        # data = normalize(data)\n",
    "\n",
    "    with Timer('Saving to {0}'.format(output_file)):\n",
    "        if not exists(kwargs['data_path']):\n",
    "            makedirs(kwargs['data_path'])\n",
    "        with h5py.File(output_file, 'w') as h5_file:\n",
    "            h5_file.attrs['number_channels'] = NUMBER_CHANNELS\n",
    "            h5_file.attrs['number_classes'] = NUMBER_OF_CLASSES\n",
    "            h5_file.attrs['version'] = H5_VERSION\n",
    "\n",
    "            # If the data is standardised standardise the training data and then use the mean and std values to\n",
    "            # standardise the validation and training\n",
    "\n",
    "            data_group = h5_file.create_group('data')\n",
    "            data_group.attrs['length_data'] = len(data)\n",
    "            data_group.create_dataset('data_channel_0', data=data, compression='gzip')\n",
    "            data_group.create_dataset('labels', data=labels, compression='gzip')\n",
    "\n",
    "\n",
    "def normalize(all_data):\n",
    "    \"\"\" normalize data \"\"\"\n",
    "    min_value = np.min(all_data)\n",
    "    max_value = np.max(all_data)\n",
    "    return (all_data - min_value) / (max_value - min_value)\n",
    "\n",
    "\n",
    "def standardize(all_data):\n",
    "    \"\"\" Standardize data \"\"\"\n",
    "    return (all_data - np.mean(all_data)) / np.std(all_data)\n",
    "\n",
    "\n",
    "def one_hot(labels, number_class):\n",
    "    \"\"\" One-hot encoding \"\"\"\n",
    "    expansion = np.eye(number_class)\n",
    "    y = expansion[:, labels].T\n",
    "    assert y.shape[1] == number_class, \"Wrong number of labels!\"\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Timer(object):\n",
    "    def __init__(self, name=None):\n",
    "        self.name = '' if name is None else name\n",
    "        self.timer = default_timer\n",
    "\n",
    "    def __enter__(self):\n",
    "        LOGGER.info('{}, Starting timer'.format(self.name))\n",
    "        self.start = self.timer()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        end = self.timer()\n",
    "        self.elapsed_secs = end - self.start\n",
    "        self.elapsed = self.elapsed_secs\n",
    "        LOGGER.info('{}, Elapsed time: {}'.format(self.name, human_time(self.elapsed)))\n",
    "\n",
    "\n",
    "class Histogram(object):\n",
    "    def __init__(self, data, bins=10, title=None, number_range=None, histogram_type='bars'):\n",
    "        self.bins = bins\n",
    "        self.title = title\n",
    "        self.type = histogram_type\n",
    "        self.histogram = np.histogram(np.array(data), bins=self.bins, range=number_range)\n",
    "        if histogram_type == 'numbers':\n",
    "            total = len(data)\n",
    "            self.percentages = [bin_value * 100.0 / total for bin_value in self.histogram[0]]\n",
    "\n",
    "    def horizontal(self, height=4, character='|'):\n",
    "        if self.title is not None:\n",
    "            his = \"{0}\\n\\n\".format(self.title)\n",
    "        else:\n",
    "            his = \"\"\n",
    "\n",
    "        if self.type == 'bars':\n",
    "            bars = self.histogram[0] / float(max(self.histogram[0])) * height\n",
    "            for reversed_height in reversed(range(1, height+1)):\n",
    "                if reversed_height == height:\n",
    "                    line = '{0} '.format(max(self.histogram[0]))\n",
    "                else:\n",
    "                    line = ' '*(len(str(max(self.histogram[0]))) + 1)\n",
    "                for c in bars:\n",
    "                    if c >= math.ceil(reversed_height):\n",
    "                        line += character\n",
    "                    else:\n",
    "                        line += ' '\n",
    "                line += '\\n'\n",
    "                his += line\n",
    "            his += '{0:.2f}'.format(self.histogram[1][0]) + ' ' * self.bins + '{0:.2f}'.format(self.histogram[1][-1]) + '\\n'\n",
    "        else:\n",
    "            his += ' ' * 4\n",
    "            his += ''.join(['| {0:^8.2f}%'.format(n) for n in self.percentages])\n",
    "            his += '|\\n'\n",
    "            his += ' ' * 4\n",
    "            his += ''.join(['| {0:^8} '.format(n) for n in self.histogram[0]])\n",
    "            his += '|\\n'\n",
    "            his += ' ' * 4\n",
    "            his += '|----------'*len(self.histogram[0])\n",
    "            his += '|\\n'\n",
    "            his += ''.join(['| {0:^8.2f} '.format(n) for n in self.histogram[1]])\n",
    "            his += '|\\n'\n",
    "        return his\n",
    "\n",
    "    def vertical(self, height=20, character='|'):\n",
    "        if self.title is not None:\n",
    "            his = \"{0}\\n\\n\".format(self.title)\n",
    "        else:\n",
    "            his = \"\"\n",
    "\n",
    "        if self.type == 'bars':\n",
    "            xl = ['{0:.2f}'.format(n) for n in self.histogram[1]]\n",
    "            lxl = [len(l) for l in xl]\n",
    "            bars = self.histogram[0] / float(max(self.histogram[0])) * height\n",
    "            bars = np.rint(bars).astype('uint32')\n",
    "            his += ' '*(max(bars)+2+max(lxl))+'{0}\\n'.format(max(self.histogram[0]))\n",
    "            for i, c in enumerate(bars):\n",
    "                line = xl[i] + ' '*(max(lxl)-lxl[i])+': ' + character*c+'\\n'\n",
    "                his += line\n",
    "        else:\n",
    "            for item1, item2, item3 in zip(self.histogram[0], self.histogram[1], self.percentages):\n",
    "                line = '{0:>6.2f} | {1:>5} | {2:>6.2f}%\\n'.format(item2, item1, item3)\n",
    "                his += line\n",
    "        return his\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GmrtLinear(nn.Module):\n",
    "    def __init__(self, keep_probability, sequence_length):\n",
    "        super(GmrtLinear, self).__init__()\n",
    "        self.keep_probability = keep_probability\n",
    "        self.input_layer_length = 6 + (sequence_length * 7)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_layer_length, HIDDEN_LAYERS).double()\n",
    "        self.fc2 = nn.Linear(HIDDEN_LAYERS + self.input_layer_length, HIDDEN_LAYERS).double()\n",
    "        self.fc3 = nn.Linear(HIDDEN_LAYERS, HIDDEN_LAYERS).double()\n",
    "        self.fc4 = nn.Linear(HIDDEN_LAYERS, HIDDEN_LAYERS).double()\n",
    "        self.fc5 = nn.Linear(HIDDEN_LAYERS, HIDDEN_LAYERS).double()\n",
    "        self.fc6 = nn.Linear(HIDDEN_LAYERS, NUMBER_OF_CLASSES).double()\n",
    "\n",
    "    def forward(self, input_data_values):\n",
    "        x = functional.leaky_relu(self.fc1(input_data_values))\n",
    "        x = functional.leaky_relu(self.fc2(torch.cat((x, input_data_values), dim=1)))\n",
    "        x = functional.dropout(x, p=self.keep_probability, training=self.training)\n",
    "        x = functional.leaky_relu(self.fc3(x))\n",
    "        x = functional.leaky_relu(self.fc4(x))\n",
    "        x = functional.dropout(x, p=self.keep_probability, training=self.training)\n",
    "        x = functional.leaky_relu(self.fc5(x))\n",
    "        x = functional.leaky_relu(self.fc6(x))\n",
    "\n",
    "        x = functional.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, rfi_data, rank=0, **kwargs):\n",
    "    # This is needed to \"trick\" numpy into using different seeds for different processes\n",
    "    if kwargs['seed'] is not None:\n",
    "        np.random.seed(kwargs['seed'] + rank)\n",
    "    else:\n",
    "        np.random.seed()\n",
    "\n",
    "    train_loader = data.DataLoader(\n",
    "        rfi_data.get_rfi_dataset('training', rank=rank, short_run_size=kwargs['short_run']),\n",
    "        batch_size=kwargs['batch_size'],\n",
    "        num_workers=3,\n",
    "        pin_memory=kwargs['using_gpu'],\n",
    "    )\n",
    "    test_loader = data.DataLoader(\n",
    "        rfi_data.get_rfi_dataset('validation', rank=rank, short_run_size=kwargs['short_run']),\n",
    "        batch_size=kwargs['batch_size'],\n",
    "        num_workers=3,\n",
    "        pin_memory=kwargs['using_gpu'],\n",
    "    )\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=kwargs['learning_rate'], momentum=kwargs['momentum'])\n",
    "    for epoch in range(1, kwargs['epochs'] + 1):\n",
    "        # Adjust the learning rate\n",
    "        adjust_learning_rate(optimizer, epoch, kwargs['learning_rate_decay'], kwargs['start_learning_rate_decay'], kwargs['learning_rate'])\n",
    "        train_epoch(epoch, model, train_loader, optimizer, kwargs['log_interval'])\n",
    "        test_epoch(model, test_loader, kwargs['log_interval'])\n",
    "\n",
    "\n",
    "def train_epoch(epoch, model, data_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (x_data_raw, target) in enumerate(data_loader):\n",
    "        x_data_raw = Variable(x_data_raw)\n",
    "        target = Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_data_raw)\n",
    "        if type(output.data) == torch.cuda.DoubleTensor:\n",
    "            output = output.cpu()\n",
    "        loss = functional.binary_cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0 and batch_idx > 1:\n",
    "            LOGGER.info('Train Epoch: {} [{}/{} ({:.0f}%)], Loss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(x_data_raw),\n",
    "                len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader),\n",
    "                loss.data[0])\n",
    "            )\n",
    "\n",
    "\n",
    "def build_histogram(output, target_column, histogram_data):\n",
    "    for values, column in zip(output.data.numpy(), target_column.numpy()):\n",
    "        histogram_data['all'].append(values[column])\n",
    "        histogram_data[column].append(values[column])\n",
    "\n",
    "\n",
    "def test_epoch(model, data_loader, log_interval):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    histogram_data = {key: [] for key in range(NUMBER_OF_CLASSES)}\n",
    "    histogram_data['all'] = []\n",
    "    for batch_index, (x_data_raw, target) in enumerate(data_loader):\n",
    "        x_data_raw = Variable(x_data_raw, volatile=True)\n",
    "        target = Variable(target)\n",
    "        output = model(x_data_raw)\n",
    "        if type(output.data) == torch.cuda.DoubleTensor:\n",
    "            output = output.cpu()\n",
    "        test_loss += functional.binary_cross_entropy(output, target).data[0]\n",
    "        pred = output.data.max(1)[1]\n",
    "        target_column = target.data.max(1)[1]\n",
    "        correct += pred.eq(target_column).sum()\n",
    "        build_histogram(output, target_column, histogram_data)\n",
    "\n",
    "        if batch_index % log_interval == 0 and batch_index > 1:\n",
    "            LOGGER.info('Test iteration: {}, Correct count: {}'.format(batch_index, correct))\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    LOGGER.info('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss,\n",
    "        correct,\n",
    "        len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset))\n",
    "    )\n",
    "    for key, value in histogram_data.items():\n",
    "        histogram = Histogram(\n",
    "            value,\n",
    "            title='Percentage of Correctly Predicted: {}'.format(key),\n",
    "            bins=10,\n",
    "            number_range=(0.0, 1.0),\n",
    "            histogram_type='numbers'\n",
    "        )\n",
    "        LOGGER.info(histogram.horizontal())\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, learning_rate_decay, start_learning_rate_decay, learning_rate):\n",
    "    \"\"\" Sets the learning rate to the initial LR decayed  \"\"\"\n",
    "    lr_decay = learning_rate_decay ** max(epoch + 1 - start_learning_rate_decay, 0.0)\n",
    "    new_learning_rate = learning_rate * lr_decay\n",
    "    LOGGER.info('New learning rate: {}'.format(new_learning_rate))\n",
    "    for param_group in optimizer.state_dict()['param_groups']:\n",
    "        param_group['lr'] = new_learning_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s:%(process)d:%(levelname)s:%(name)s:%(message)s')\n",
    "    parser = argparse.ArgumentParser(description='GMRT DNN Training')\n",
    "    parser.add_argument('--batch_size', type=int, default=20000, metavar='N', help='input batch size for training (default: 20000)')\n",
    "    parser.add_argument('--epochs', type=int, default=5, metavar='N', help='number of epochs to train (default: 5)')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01, metavar='LR', help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, metavar='M', help='SGD momentum (default: 0.5)')\n",
    "    parser.add_argument('--keep_probability', type=float, default=0.6, metavar='K', help='Dropout keep probability (default: 0.6)')\n",
    "    parser.add_argument('--log_interval', type=int, default=10, metavar='N', help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--num_processes', type=int, default=4, metavar='N', help='how many training processes to use (default: 4)')\n",
    "    parser.add_argument('--use_gpu', action='store_true', default=False, help='use the GPU if it is available')\n",
    "    parser.add_argument('--data_path', default='/tmp', help='the path to the data file')\n",
    "    parser.add_argument('--data_file', default='data.h5', help='the name of the data file')\n",
    "    parser.add_argument('--sequence_length', type=int, default=10, help='how many elements in a sequence')\n",
    "    parser.add_argument('--validation_percentage', type=int, default=10, help='amount of data used for validation')\n",
    "    parser.add_argument('--training_percentage', type=int, default=80, help='amount of data used for training')\n",
    "    parser.add_argument('--seed', type=int, default=None, metavar='S', help='random seed (default: 1)')\n",
    "    parser.add_argument('--learning_rate_decay', type=float, default=0.8, metavar='LRD', help='the initial learning rate decay rate')\n",
    "    parser.add_argument('--start_learning_rate_decay', type=int, default=5, help='the epoch to start applying the LRD')\n",
    "    parser.add_argument('--short_run', type=int, default=None, help='use a short run of the test data')\n",
    "    parser.add_argument('--save', type=str,  default=None, help='path to save the final model')\n",
    "    parser.add_argument('--dataset', type=int,  default=1, help='the dataset you wish to run')\n",
    "\n",
    "    kwargs = vars(parser.parse_args())\n",
    "    LOGGER.debug(kwargs)\n",
    "\n",
    "    # If the have specified a seed get a random\n",
    "    if kwargs['seed'] is not None:\n",
    "        np.random.seed(kwargs['seed'])\n",
    "    else:\n",
    "        np.random.seed()\n",
    "\n",
    "    if kwargs['use_gpu'] and torch.cuda.is_available():\n",
    "        LOGGER.info('Using cuda devices: {}'.format(torch.cuda.device_count()))\n",
    "        kwargs['cuda_device_count'] = torch.cuda.device_count()\n",
    "        kwargs['using_gpu'] = True\n",
    "    else:\n",
    "        LOGGER.info('Using CPU')\n",
    "        kwargs['cuda_device_count'] = 0\n",
    "        kwargs['using_gpu'] = False\n",
    "\n",
    "    # Do this first so all the data is built before we go parallel and get race conditions\n",
    "    with Timer('Checking/Building data file'):\n",
    "        build_data(**kwargs)\n",
    "\n",
    "    rfi_data = RfiData(**kwargs)\n",
    "\n",
    "    if kwargs['using_gpu']:\n",
    "        # The DataParallel will distribute the model to all the available GPUs\n",
    "        model = nn.DataParallel(GmrtLinear(kwargs['keep_probability'], kwargs['sequence_length'])).cuda()\n",
    "\n",
    "        # Train\n",
    "        train(model, rfi_data, **kwargs)\n",
    "\n",
    "    else:\n",
    "        # This uses the HOGWILD! approach to lock free SGD\n",
    "        model = GmrtLinear(kwargs['keep_probability'], kwargs['sequence_length'])\n",
    "        model.share_memory()  # gradients are allocated lazily, so they are not shared here\n",
    "\n",
    "        processes = []\n",
    "        for rank in range(kwargs['num_processes']):\n",
    "            p = mp.Process(target=train, args=(model, rfi_data, rank), kwargs=kwargs)\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "\n",
    "    with Timer('Reading final test data'):\n",
    "        test_loader = data.DataLoader(\n",
    "            rfi_data.get_rfi_dataset('test', short_run_size=kwargs['short_run']),\n",
    "            batch_size=kwargs['batch_size'],\n",
    "            num_workers=3,\n",
    "            pin_memory=kwargs['using_gpu'],\n",
    "        )\n",
    "    with Timer('Final test'):\n",
    "        test_epoch(model, test_loader, kwargs['log_interval'])\n",
    "\n",
    "    if kwargs['save'] is not None:\n",
    "        with Timer('Saving model'):\n",
    "            with open(kwargs['save'], 'wb') as save_file:\n",
    "                torch.save(model.state_dict(), save_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-28 08:22:25,559:54646:DEBUG:__main__:{'save': None, 'data_path': '/tmp', 'data_file': 'data.h5', 'use_gpu': False, 'learning_rate': 0.01, 'num_processes': 4, 'log_interval': 10, 'dataset': 1, 'epochs': 5, 'seed': None, 'start_learning_rate_decay': 5, 'batch_size': 20000, 'validation_percentage': 10, 'sequence_length': 10, 'short_run': None, 'learning_rate_decay': 0.8, 'training_percentage': 80, 'momentum': 0.5, 'keep_probability': 0.6}\n",
      "2017-11-28 08:22:25,566:54646:INFO:__main__:Using CPU\n",
      "2017-11-28 08:22:25,567:54646:INFO:__main__:Checking/Building data file, Starting timer\n",
      "2017-11-28 08:22:25,568:54646:INFO:__main__:Processing input files, Starting timer\n",
      "2017-11-28 08:22:25,568:54646:INFO:__main__:Loading: http://ict.icrar.org/store/staff/kevin/rfi/impulsive_broadband_simulation_random_5p.txt\n",
      "2017-11-28 08:22:34,812:54646:INFO:__main__:Loading: http://ict.icrar.org/store/staff/kevin/rfi/impulsive_broadband_simulation_random_5p_loc.txt\n",
      "2017-11-28 08:22:37,477:54646:INFO:__main__:Loading: http://ict.icrar.org/store/staff/kevin/rfi/impulsive_broadband_simulation_random_10p.txt\n",
      "2017-11-28 08:22:45,502:54646:INFO:__main__:Loading: http://ict.icrar.org/store/staff/kevin/rfi/impulsive_broadband_simulation_random_10p_loc.txt\n",
      "2017-11-28 08:22:48,115:54646:INFO:__main__:Loading: http://ict.icrar.org/store/staff/kevin/rfi/repetitive_rfi_timeseries.txt\n",
      "2017-11-28 08:22:49,033:54646:INFO:__main__:Loading: http://ict.icrar.org/store/staff/kevin/rfi/repetitive_rfi_timeseries_loc.txt\n",
      "2017-11-28 08:22:49,320:54646:INFO:__main__:Loading: http://ict.icrar.org/store/staff/kevin/rfi/repetitive_rfi_random_timeseries.txt\n",
      "2017-11-28 08:22:50,057:54646:INFO:__main__:Loading: http://ict.icrar.org/store/staff/kevin/rfi/repetitive_rfi_random_timeseries_loc.txt\n",
      "2017-11-28 08:22:50,347:54646:INFO:__main__:Processing input files, Elapsed time:    24s\n",
      "2017-11-28 08:22:50,348:54646:INFO:__main__:Concatenating data, Starting timer\n",
      "2017-11-28 08:22:50,589:54646:INFO:__main__:Concatenating data, Elapsed time:     0s\n",
      "2017-11-28 08:22:50,590:54646:INFO:__main__:Standardise & One hot, Starting timer\n",
      "2017-11-28 08:22:51,157:54646:INFO:__main__:Standardise & One hot, Elapsed time:     0s\n",
      "2017-11-28 08:22:51,158:54646:INFO:__main__:Saving to /tmp/data.h5, Starting timer\n",
      "2017-11-28 08:23:06,840:54646:INFO:__main__:Saving to /tmp/data.h5, Elapsed time:    15s\n",
      "2017-11-28 08:23:06,930:54646:INFO:__main__:Checking/Building data file, Elapsed time:    41s\n",
      "2017-11-28 08:23:15,210:54673:DEBUG:__main__:Length: 4423678\n",
      "2017-11-28 08:23:15,335:54671:DEBUG:__main__:Length: 4423678\n",
      "2017-11-28 08:23:15,337:54672:DEBUG:__main__:Length: 4423678\n",
      "2017-11-28 08:23:15,430:54674:DEBUG:__main__:Length: 4423678\n",
      "2017-11-28 08:23:16,978:54673:DEBUG:__main__:Length: 552959\n",
      "2017-11-28 08:23:16,985:54673:INFO:__main__:New learning rate: 0.01\n",
      "2017-11-28 08:23:17,049:54672:DEBUG:__main__:Length: 552959\n",
      "2017-11-28 08:23:17,055:54672:INFO:__main__:New learning rate: 0.01\n",
      "2017-11-28 08:23:17,076:54671:DEBUG:__main__:Length: 552959\n",
      "2017-11-28 08:23:17,080:54671:INFO:__main__:New learning rate: 0.01\n",
      "2017-11-28 08:23:17,323:54674:DEBUG:__main__:Length: 552962\n",
      "2017-11-28 08:23:17,328:54674:INFO:__main__:New learning rate: 0.01\n",
      "2017-11-28 08:24:55,471:54673:INFO:__main__:Train Epoch: 1 [200000/4423678 (5%)], Loss: 0.297135\n",
      "2017-11-28 08:24:55,483:54672:INFO:__main__:Train Epoch: 1 [200000/4423678 (5%)], Loss: 0.312224\n",
      "2017-11-28 08:24:57,647:54671:INFO:__main__:Train Epoch: 1 [200000/4423678 (5%)], Loss: 0.309933\n",
      "2017-11-28 08:24:58,209:54674:INFO:__main__:Train Epoch: 1 [200000/4423678 (5%)], Loss: 0.293617\n",
      "2017-11-28 08:26:07,920:54673:INFO:__main__:Train Epoch: 1 [400000/4423678 (9%)], Loss: 0.276536\n",
      "2017-11-28 08:26:08,246:54672:INFO:__main__:Train Epoch: 1 [400000/4423678 (9%)], Loss: 0.270132\n",
      "2017-11-28 08:26:09,741:54671:INFO:__main__:Train Epoch: 1 [400000/4423678 (9%)], Loss: 0.275205\n",
      "2017-11-28 08:26:09,874:54674:INFO:__main__:Train Epoch: 1 [400000/4423678 (9%)], Loss: 0.260342\n",
      "2017-11-28 08:27:31,765:54672:INFO:__main__:Train Epoch: 1 [600000/4423678 (14%)], Loss: 0.246210\n",
      "2017-11-28 08:27:31,844:54671:INFO:__main__:Train Epoch: 1 [600000/4423678 (14%)], Loss: 0.251940\n",
      "2017-11-28 08:27:32,356:54673:INFO:__main__:Train Epoch: 1 [600000/4423678 (14%)], Loss: 0.242799\n",
      "2017-11-28 08:27:33,042:54674:INFO:__main__:Train Epoch: 1 [600000/4423678 (14%)], Loss: 0.244204\n",
      "2017-11-28 08:28:43,911:54672:INFO:__main__:Train Epoch: 1 [800000/4423678 (18%)], Loss: 0.215488\n",
      "2017-11-28 08:28:45,045:54673:INFO:__main__:Train Epoch: 1 [800000/4423678 (18%)], Loss: 0.217960\n",
      "2017-11-28 08:28:46,229:54671:INFO:__main__:Train Epoch: 1 [800000/4423678 (18%)], Loss: 0.219238\n",
      "2017-11-28 08:28:47,655:54674:INFO:__main__:Train Epoch: 1 [800000/4423678 (18%)], Loss: 0.224956\n",
      "2017-11-28 08:29:56,409:54673:INFO:__main__:Train Epoch: 1 [1000000/4423678 (23%)], Loss: 0.192991\n",
      "2017-11-28 08:29:56,579:54672:INFO:__main__:Train Epoch: 1 [1000000/4423678 (23%)], Loss: 0.192608\n",
      "2017-11-28 08:29:57,620:54671:INFO:__main__:Train Epoch: 1 [1000000/4423678 (23%)], Loss: 0.185027\n",
      "2017-11-28 08:29:58,346:54674:INFO:__main__:Train Epoch: 1 [1000000/4423678 (23%)], Loss: 0.189632\n",
      "2017-11-28 08:31:09,213:54672:INFO:__main__:Train Epoch: 1 [1200000/4423678 (27%)], Loss: 0.163157\n",
      "2017-11-28 08:31:09,599:54671:INFO:__main__:Train Epoch: 1 [1200000/4423678 (27%)], Loss: 0.168911\n",
      "2017-11-28 08:31:10,310:54673:INFO:__main__:Train Epoch: 1 [1200000/4423678 (27%)], Loss: 0.171085\n",
      "2017-11-28 08:31:10,759:54674:INFO:__main__:Train Epoch: 1 [1200000/4423678 (27%)], Loss: 0.164399\n",
      "2017-11-28 08:32:17,271:54672:INFO:__main__:Train Epoch: 1 [1400000/4423678 (32%)], Loss: 0.141734\n",
      "2017-11-28 08:32:18,765:54673:INFO:__main__:Train Epoch: 1 [1400000/4423678 (32%)], Loss: 0.139113\n",
      "2017-11-28 08:32:19,462:54671:INFO:__main__:Train Epoch: 1 [1400000/4423678 (32%)], Loss: 0.133214\n",
      "2017-11-28 08:32:20,951:54674:INFO:__main__:Train Epoch: 1 [1400000/4423678 (32%)], Loss: 0.146423\n",
      "2017-11-28 08:33:22,944:54672:INFO:__main__:Train Epoch: 1 [1600000/4423678 (36%)], Loss: 0.123425\n",
      "2017-11-28 08:33:23,185:54673:INFO:__main__:Train Epoch: 1 [1600000/4423678 (36%)], Loss: 0.123194\n",
      "2017-11-28 08:33:23,764:54671:INFO:__main__:Train Epoch: 1 [1600000/4423678 (36%)], Loss: 0.118972\n",
      "2017-11-28 08:33:24,609:54674:INFO:__main__:Train Epoch: 1 [1600000/4423678 (36%)], Loss: 0.126424\n",
      "2017-11-28 08:34:37,906:54672:INFO:__main__:Train Epoch: 1 [1800000/4423678 (41%)], Loss: 0.113409\n",
      "2017-11-28 08:34:38,282:54671:INFO:__main__:Train Epoch: 1 [1800000/4423678 (41%)], Loss: 0.115549\n",
      "2017-11-28 08:34:39,676:54674:INFO:__main__:Train Epoch: 1 [1800000/4423678 (41%)], Loss: 0.117350\n",
      "2017-11-28 08:34:39,772:54673:INFO:__main__:Train Epoch: 1 [1800000/4423678 (41%)], Loss: 0.114297\n",
      "2017-11-28 08:35:55,348:54672:INFO:__main__:Train Epoch: 1 [2000000/4423678 (45%)], Loss: 0.106341\n",
      "2017-11-28 08:35:57,260:54673:INFO:__main__:Train Epoch: 1 [2000000/4423678 (45%)], Loss: 0.100544\n",
      "2017-11-28 08:35:57,267:54671:INFO:__main__:Train Epoch: 1 [2000000/4423678 (45%)], Loss: 0.109240\n",
      "2017-11-28 08:35:59,410:54674:INFO:__main__:Train Epoch: 1 [2000000/4423678 (45%)], Loss: 0.102191\n",
      "2017-11-28 08:36:59,385:54672:INFO:__main__:Train Epoch: 1 [2200000/4423678 (50%)], Loss: 0.096273\n",
      "2017-11-28 08:36:59,782:54673:INFO:__main__:Train Epoch: 1 [2200000/4423678 (50%)], Loss: 0.102592\n",
      "2017-11-28 08:37:00,154:54671:INFO:__main__:Train Epoch: 1 [2200000/4423678 (50%)], Loss: 0.097687\n",
      "2017-11-28 08:37:01,499:54674:INFO:__main__:Train Epoch: 1 [2200000/4423678 (50%)], Loss: 0.097077\n",
      "2017-11-28 08:38:07,878:54672:INFO:__main__:Train Epoch: 1 [2400000/4423678 (54%)], Loss: 0.091667\n",
      "2017-11-28 08:38:08,785:54671:INFO:__main__:Train Epoch: 1 [2400000/4423678 (54%)], Loss: 0.092780\n",
      "2017-11-28 08:38:10,254:54674:INFO:__main__:Train Epoch: 1 [2400000/4423678 (54%)], Loss: 0.092897\n",
      "2017-11-28 08:38:10,758:54673:INFO:__main__:Train Epoch: 1 [2400000/4423678 (54%)], Loss: 0.098104\n",
      "2017-11-28 08:39:10,802:54672:INFO:__main__:Train Epoch: 1 [2600000/4423678 (59%)], Loss: 0.089630\n",
      "2017-11-28 08:39:12,512:54671:INFO:__main__:Train Epoch: 1 [2600000/4423678 (59%)], Loss: 0.087428\n",
      "2017-11-28 08:39:12,696:54673:INFO:__main__:Train Epoch: 1 [2600000/4423678 (59%)], Loss: 0.094955\n",
      "2017-11-28 08:39:15,025:54674:INFO:__main__:Train Epoch: 1 [2600000/4423678 (59%)], Loss: 0.090962\n",
      "2017-11-28 08:40:14,098:54672:INFO:__main__:Train Epoch: 1 [2800000/4423678 (63%)], Loss: 0.088399\n",
      "2017-11-28 08:40:14,453:54673:INFO:__main__:Train Epoch: 1 [2800000/4423678 (63%)], Loss: 0.082525\n",
      "2017-11-28 08:40:14,812:54671:INFO:__main__:Train Epoch: 1 [2800000/4423678 (63%)], Loss: 0.084222\n",
      "2017-11-28 08:40:16,204:54674:INFO:__main__:Train Epoch: 1 [2800000/4423678 (63%)], Loss: 0.082214\n",
      "2017-11-28 08:41:23,600:54672:INFO:__main__:Train Epoch: 1 [3000000/4423678 (68%)], Loss: 0.088461\n",
      "2017-11-28 08:41:24,987:54671:INFO:__main__:Train Epoch: 1 [3000000/4423678 (68%)], Loss: 0.086320\n",
      "2017-11-28 08:41:26,303:54674:INFO:__main__:Train Epoch: 1 [3000000/4423678 (68%)], Loss: 0.081510\n",
      "2017-11-28 08:41:27,094:54673:INFO:__main__:Train Epoch: 1 [3000000/4423678 (68%)], Loss: 0.086303\n",
      "2017-11-28 08:42:25,919:54672:INFO:__main__:Train Epoch: 1 [3200000/4423678 (72%)], Loss: 0.080770\n",
      "2017-11-28 08:42:27,422:54671:INFO:__main__:Train Epoch: 1 [3200000/4423678 (72%)], Loss: 0.080747\n",
      "2017-11-28 08:42:28,152:54673:INFO:__main__:Train Epoch: 1 [3200000/4423678 (72%)], Loss: 0.080298\n",
      "2017-11-28 08:42:30,597:54674:INFO:__main__:Train Epoch: 1 [3200000/4423678 (72%)], Loss: 0.082413\n",
      "2017-11-28 08:43:29,923:54672:INFO:__main__:Train Epoch: 1 [3400000/4423678 (77%)], Loss: 0.080710\n",
      "2017-11-28 08:43:30,277:54671:INFO:__main__:Train Epoch: 1 [3400000/4423678 (77%)], Loss: 0.078614\n",
      "2017-11-28 08:43:30,287:54673:INFO:__main__:Train Epoch: 1 [3400000/4423678 (77%)], Loss: 0.081081\n",
      "2017-11-28 08:43:32,107:54674:INFO:__main__:Train Epoch: 1 [3400000/4423678 (77%)], Loss: 0.082021\n",
      "2017-11-28 08:44:38,860:54672:INFO:__main__:Train Epoch: 1 [3600000/4423678 (81%)], Loss: 0.082772\n",
      "2017-11-28 08:44:40,056:54671:INFO:__main__:Train Epoch: 1 [3600000/4423678 (81%)], Loss: 0.075093\n",
      "2017-11-28 08:44:41,557:54674:INFO:__main__:Train Epoch: 1 [3600000/4423678 (81%)], Loss: 0.079189\n",
      "2017-11-28 08:44:43,077:54673:INFO:__main__:Train Epoch: 1 [3600000/4423678 (81%)], Loss: 0.071973\n",
      "2017-11-28 08:45:46,131:54672:INFO:__main__:Train Epoch: 1 [3800000/4423678 (86%)], Loss: 0.077037\n",
      "2017-11-28 08:45:47,591:54671:INFO:__main__:Train Epoch: 1 [3800000/4423678 (86%)], Loss: 0.072200\n",
      "2017-11-28 08:45:48,664:54673:INFO:__main__:Train Epoch: 1 [3800000/4423678 (86%)], Loss: 0.080913\n",
      "2017-11-28 08:45:51,547:54674:INFO:__main__:Train Epoch: 1 [3800000/4423678 (86%)], Loss: 0.075729\n",
      "2017-11-28 08:46:56,634:54672:INFO:__main__:Train Epoch: 1 [4000000/4423678 (90%)], Loss: 0.079546\n",
      "2017-11-28 08:46:56,859:54671:INFO:__main__:Train Epoch: 1 [4000000/4423678 (90%)], Loss: 0.072656\n",
      "2017-11-28 08:46:57,214:54673:INFO:__main__:Train Epoch: 1 [4000000/4423678 (90%)], Loss: 0.075734\n",
      "2017-11-28 08:46:59,001:54674:INFO:__main__:Train Epoch: 1 [4000000/4423678 (90%)], Loss: 0.074924\n",
      "2017-11-28 08:48:05,009:54672:INFO:__main__:Train Epoch: 1 [4200000/4423678 (95%)], Loss: 0.075128\n",
      "2017-11-28 08:48:06,350:54671:INFO:__main__:Train Epoch: 1 [4200000/4423678 (95%)], Loss: 0.075184\n",
      "2017-11-28 08:48:07,889:54674:INFO:__main__:Train Epoch: 1 [4200000/4423678 (95%)], Loss: 0.069456\n",
      "2017-11-28 08:48:09,627:54673:INFO:__main__:Train Epoch: 1 [4200000/4423678 (95%)], Loss: 0.075428\n",
      "2017-11-28 08:48:56,203:54672:INFO:__main__:Train Epoch: 1 [4400000/4423678 (99%)], Loss: 0.075811\n",
      "2017-11-28 08:48:57,067:54671:INFO:__main__:Train Epoch: 1 [4400000/4423678 (99%)], Loss: 0.068310\n",
      "2017-11-28 08:48:58,494:54673:INFO:__main__:Train Epoch: 1 [4400000/4423678 (99%)], Loss: 0.071887\n",
      "2017-11-28 08:49:00,245:54674:INFO:__main__:Train Epoch: 1 [4400000/4423678 (99%)], Loss: 0.070256\n",
      "Process Process-1:6:\n",
      "Process Process-3:6:\n",
      "Process Process-4:4:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-3:4:\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "    self.run()\n",
      "    self.run()\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "    r = index_queue.get()\n",
      "    self.run()\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/queues.py\", line 378, in get\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/queues.py\", line 378, in get\n",
      "    return recv()\n",
      "    return recv()\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/multiprocessing/queue.py\", line 21, in recv\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/multiprocessing/queue.py\", line 21, in recv\n",
      "    self.run()\n",
      "    buf = self.recv_bytes()\n",
      "    buf = self.recv_bytes()\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-2-ac42dfb7ccb4>\", line 79, in __getitem__\n",
      "  File \"<ipython-input-2-ac42dfb7ccb4>\", line 79, in __getitem__\n",
      "Process Process-3:5:\n",
      "    local_median_absolute_deviation = scale.mad(x_data, c=1)\n",
      "    local_median_absolute_deviation = scale.mad(x_data, c=1)\n",
      "Process Process-1:4:\n",
      "Process Process-2:4:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-2:6:\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "Process Process-2:5:\n",
      "    self.run()\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self.run()\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self.run()\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self.run()\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "  File \"<ipython-input-2-ac42dfb7ccb4>\", line 79, in __getitem__\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/statsmodels/robust/scale.py\", line 43, in mad\n",
      "    local_median_absolute_deviation = scale.mad(x_data, c=1)\n",
      "  File \"<ipython-input-2-ac42dfb7ccb4>\", line 79, in __getitem__\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "Process Process-1:5:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-ac42dfb7ccb4>\", line 78, in __getitem__\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/statsmodels/robust/scale.py\", line 44, in mad\n",
      "    local_median_absolute_deviation = scale.mad(x_data, c=1)\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/statsmodels/robust/scale.py\", line 44, in mad\n",
      "    local_median = np.median(x_data)\n",
      "  File \"<ipython-input-2-ac42dfb7ccb4>\", line 85, in __getitem__\n",
      "    return np.median((np.fabs(a-center))/c, axis=axis)\n",
      "Process Process-4:5:\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/numpy/lib/function_base.py\", line 4097, in median\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/numpy/lib/function_base.py\", line 4097, in median\n",
      "    return np.median((np.fabs(a-center))/c, axis=axis)\n",
      "    data.append(item)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/numpy/lib/function_base.py\", line 4097, in median\n",
      "KeyboardInterrupt\n",
      "    overwrite_input=overwrite_input)\n",
      "    overwrite_input=overwrite_input)\n",
      "    self.run()\n",
      "    overwrite_input=overwrite_input)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/numpy/lib/function_base.py\", line 4011, in _ureduce\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/numpy/lib/function_base.py\", line 4011, in _ureduce\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/numpy/lib/function_base.py\", line 4011, in _ureduce\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    r = func(a, **kwargs)\n",
      "    r = func(a, **kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/numpy/lib/function_base.py\", line 4130, in _median\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/numpy/lib/function_base.py\", line 4150, in _median\n",
      "    self.run()\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/statsmodels/robust/scale.py\", line 43, in mad\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    return np.lib.utils._median_nancheck(part, rout, axis, out)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/numpy/lib/utils.py\", line 1141, in _median_nancheck\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "    data = np.rollaxis(data, axis, data.ndim)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"<ipython-input-2-ac42dfb7ccb4>\", line 79, in __getitem__\n",
      "KeyboardInterrupt\n",
      "    self.run()\n",
      "    local_median_absolute_deviation = scale.mad(x_data, c=1)\n",
      "Process Process-4:\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/statsmodels/robust/scale.py\", line 43, in mad\n",
      "Traceback (most recent call last):\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-5-3d0782b53df9>\", line 26, in train\n",
      "    center = np.apply_over_axes(center, a, axis)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/numpy/lib/shape_base.py\", line 231, in apply_over_axes\n",
      "    test_epoch(model, test_loader, kwargs['log_interval'])\n",
      "    self.run()\n",
      "    res = expand_dims(res, axis)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"<ipython-input-5-3d0782b53df9>\", line 68, in test_epoch\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    output = model(x_data_raw)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/numpy/lib/shape_base.py\", line 310, in expand_dims\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 224, in __call__\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c9d31999d1e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'program_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-98bad1da31cd>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mprocesses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reading final test data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.pyc\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0m_current_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/forking.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0mdeadline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mdelay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0005\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/forking.pyc\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    output = model(x_data_raw)\n",
      "  File \"<ipython-input-5-3d0782b53df9>\", line 26, in train\n",
      "Process Process-3:\n",
      "    test_epoch(model, test_loader, kwargs['log_interval'])\n",
      "Traceback (most recent call last):\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-5-3d0782b53df9>\", line 68, in test_epoch\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"<ipython-input-4-be6a308c09e1>\", line 18, in forward\n",
      "    output = model(x_data_raw)\n",
      "    x = functional.leaky_relu(self.fc3(x))\n",
      "    self.run()\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 224, in __call__\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 224, in __call__\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    result = self.forward(*input, **kwargs)\n",
      "    result = self.forward(*input, **kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-be6a308c09e1>\", line 16, in forward\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/nn/modules/linear.py\", line 53, in forward\n",
      "  File \"<ipython-input-5-3d0782b53df9>\", line 26, in train\n",
      "    x = functional.leaky_relu(self.fc2(torch.cat((x, input_data_values), dim=1)))\n",
      "    test_epoch(model, test_loader, kwargs['log_interval'])\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 224, in __call__\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/nn/functional.py\", line 553, in linear\n",
      "  File \"<ipython-input-5-3d0782b53df9>\", line 68, in test_epoch\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/nn/modules/linear.py\", line 53, in forward\n",
      "    return torch.addmm(bias, input, weight.t())\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 224, in __call__\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/autograd/variable.py\", line 924, in addmm\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/nn/functional.py\", line 553, in linear\n",
      "  File \"<ipython-input-4-be6a308c09e1>\", line 16, in forward\n",
      "    return cls._blas(Addmm, args, False)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/autograd/variable.py\", line 920, in _blas\n",
      "    return torch.addmm(bias, input, weight.t())\n",
      "    x = functional.leaky_relu(self.fc2(torch.cat((x, input_data_values), dim=1)))\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/autograd/variable.py\", line 924, in addmm\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 224, in __call__\n",
      "    return cls.apply(*(tensors + (alpha, beta, inplace)))\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/autograd/_functions/blas.py\", line 26, in forward\n",
      "    result = self.forward(*input, **kwargs)\n",
      "    return cls._blas(Addmm, args, False)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/autograd/variable.py\", line 920, in _blas\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/nn/modules/linear.py\", line 53, in forward\n",
      "    matrix1, matrix2, out=output)\n",
      "Process Process-1:\n",
      "    return cls.apply(*(tensors + (alpha, beta, inplace)))\n",
      "KeyboardInterrupt\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/autograd/_functions/blas.py\", line 26, in forward\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/nn/functional.py\", line 553, in linear\n",
      "    matrix1, matrix2, out=output)\n",
      "    self.run()\n",
      "    return torch.addmm(bias, input, weight.t())\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/autograd/variable.py\", line 924, in addmm\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-5-3d0782b53df9>\", line 26, in train\n",
      "    return cls._blas(Addmm, args, False)\n",
      "    test_epoch(model, test_loader, kwargs['log_interval'])\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/autograd/variable.py\", line 920, in _blas\n",
      "  File \"<ipython-input-5-3d0782b53df9>\", line 75, in test_epoch\n",
      "    return cls.apply(*(tensors + (alpha, beta, inplace)))\n",
      "    build_histogram(output, target_column, histogram_data)\n",
      "  File \"/Users/kevinvinsen/anaconda2/envs/rfi_ml/lib/python2.7/site-packages/torch/autograd/_functions/blas.py\", line 26, in forward\n",
      "  File \"<ipython-input-5-3d0782b53df9>\", line 53, in build_histogram\n",
      "    matrix1, matrix2, out=output)\n",
      "    for values, column in zip(output.data.numpy(), target_column.numpy()):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = ['program_name']\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 (rfi_ml)",
   "language": "python",
   "name": "rfi_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
