{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Spotting RFI in GMRT data\n",
    "\n",
    "This tutorial will show you how to use GMRT simulated data to spot RFI from \"normal\" noise.\n",
    "\n",
    "Firstly, I must thank Kaushal Buch from NCRA in India for letting me share the data with you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some standard imports and constants done first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from os import makedirs\n",
    "from os.path import exists\n",
    "import sys\n",
    "from timeit import default_timer\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from astropy.utils.console import human_time\n",
    "from statsmodels.robust import scale\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "H5_VERSION = '2017_10_20_001'\n",
    "NUMBER_CHANNELS = 1\n",
    "NUMBER_OF_CLASSES = 2\n",
    "URL_ROOT = 'http://ict.icrar.org/store/staff/kevin/rfi'\n",
    "HIDDEN_LAYERS = 200\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in a series of files stored on the ICRAR \"shared\" area. The files come as a pair <filename>.txt contains the voltage at the antenna, whilst <filename>_loc.txt contains 0 or 1 for the corresponding row to show whether the voltage is no RFI or RFI.\n",
    "\n",
    "Reading the text files is fastest in Pandas, but storing the data as Numpy arrays is fastest in HDF5 (h5py).\n",
    "\n",
    "We have four files representing different noise conditions.\n",
    "\n",
    "The HDF5 file stores the data as a series of points, with two classes. It is RFI or it isn't RFI. Once the HDF5 file has been created we can keep using it.\n",
    "\n",
    "The code is designed to work with Single Processes, Multiple Process (via the Hogwild algorithm) and GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class H5Exception(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class RfiData(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self._sequence_length = kwargs['sequence_length']\n",
    "        self._num_processes = kwargs['num_processes']\n",
    "        self._using_gpu = kwargs['using_gpu']\n",
    "        output_file = os.path.join(kwargs['data_path'], kwargs['data_file'])\n",
    "        with h5py.File(output_file, 'r') as h5_file:\n",
    "            data_group = h5_file['data']\n",
    "\n",
    "            # Move the data into memory\n",
    "            self._data_channel_0 = np.copy(data_group['data_channel_0'])\n",
    "            self._labels = np.copy(data_group['labels'])\n",
    "\n",
    "            length_data = len(self._labels) - kwargs['sequence_length']\n",
    "            split_point1 = int(length_data * kwargs['training_percentage'] / 100.)\n",
    "            split_point2 = int(length_data * (kwargs['training_percentage'] + kwargs['validation_percentage']) / 100.)\n",
    "            perm0 = np.arange(length_data)\n",
    "            np.random.shuffle(perm0)\n",
    "\n",
    "            self._train_sequence = perm0[:split_point1]\n",
    "            self._validation_sequence = perm0[split_point1:split_point2]\n",
    "            self._test_sequence = perm0[split_point2:]\n",
    "\n",
    "    def get_rfi_dataset(self, data_type, rank=None, short_run_size=None):\n",
    "        if data_type not in ['training', 'validation', 'test']:\n",
    "            raise ValueError(\"data_type must be one of: 'training', 'validation', 'test'\")\n",
    "\n",
    "        if data_type == 'training':\n",
    "            sequence = self._train_sequence\n",
    "        elif data_type == 'validation':\n",
    "            sequence = self._validation_sequence\n",
    "        else:\n",
    "            sequence = self._test_sequence\n",
    "\n",
    "        if self._using_gpu or rank is None:\n",
    "            if short_run_size is not None:\n",
    "                sequence = sequence[0:short_run_size]\n",
    "        else:\n",
    "            section_length = len(sequence) / self._num_processes\n",
    "            start = rank * section_length\n",
    "            if rank == self._num_processes - 1:\n",
    "                if short_run_size is not None:\n",
    "                    sequence = sequence[start:start + short_run_size]\n",
    "                else:\n",
    "                    sequence = sequence[start:]\n",
    "            else:\n",
    "                if short_run_size is not None:\n",
    "                    sequence = sequence[start:start + short_run_size]\n",
    "                else:\n",
    "                    sequence = sequence[start:start + section_length]\n",
    "\n",
    "        return RfiDataset(sequence, self._data_channel_0, self._labels, self._sequence_length)\n",
    "\n",
    "\n",
    "class RfiDataset(Dataset):\n",
    "    def __init__(self, selection_order, x_data, y_data, sequence_length):\n",
    "        self._x_data = x_data\n",
    "        self._y_data = y_data\n",
    "        self._selection_order = selection_order\n",
    "        self._length = len(selection_order)\n",
    "        self._sequence_length = sequence_length\n",
    "        self._actual_node = self._sequence_length / 2\n",
    "        self._median = np.median(x_data)\n",
    "        self._median_absolute_deviation = scale.mad(x_data, c=1)\n",
    "        self._mean = np.mean(x_data)\n",
    "        LOGGER.debug('Length: {}'.format(self._length))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        selection_index = self._selection_order[index]\n",
    "        x_data = self._x_data[selection_index:selection_index + self._sequence_length]\n",
    "        local_median = np.median(x_data)\n",
    "        local_median_absolute_deviation = scale.mad(x_data, c=1)\n",
    "        local_mean = np.mean(x_data)\n",
    "        # x_data_last = x_data[self._actual_node]\n",
    "\n",
    "        data = [self._median, self._median_absolute_deviation, self._mean, local_median, local_median_absolute_deviation, local_mean]\n",
    "        for item in x_data:\n",
    "            data.append(item)\n",
    "            data.append(item - self._mean)\n",
    "            data.append(item - self._median)\n",
    "            data.append(item - self._median_absolute_deviation)\n",
    "            data.append(item - local_mean)\n",
    "            data.append(item - local_median)\n",
    "            data.append(item - local_median_absolute_deviation)\n",
    "\n",
    "        # return np.reshape(x_data, (NUMBER_CHANNELS, -1)), values, self._y_data[selection_index + self._actual_node]\n",
    "        return np.array(data), self._y_data[selection_index + self._actual_node]\n",
    "\n",
    "\n",
    "def process_files(filename, rfi_label):\n",
    "    \"\"\" Process a file and return the data and the labels \"\"\"\n",
    "    files_to_process = []\n",
    "    for ending in ['.txt', '_loc.txt']:\n",
    "        complete_filename = filename + ending\n",
    "        files_to_process.append(complete_filename)\n",
    "\n",
    "    # Load the files into numpy\n",
    "    LOGGER.info('Loading: {}'.format(files_to_process[0]))\n",
    "    data_frame = pd.read_csv(files_to_process[0], header=None, delimiter=' ')\n",
    "    data = data_frame.values.flatten()\n",
    "\n",
    "    LOGGER.info('Loading: {}'.format(files_to_process[1]))\n",
    "    data_frame = pd.read_csv(files_to_process[1], header=None, delimiter=' ')\n",
    "    labels = data_frame.values.flatten()\n",
    "\n",
    "    # Check the lengths match\n",
    "    assert len(data) == len(labels), 'The line counts do not match for: {0}'.format(filename)\n",
    "\n",
    "    # If substitute of the label is needed\n",
    "    if rfi_label != 1:\n",
    "        labels[labels == 1] = rfi_label\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def build_data(**kwargs):\n",
    "    \"\"\" Read data \"\"\"\n",
    "    output_file = os.path.join(kwargs['data_path'], kwargs['data_file'])\n",
    "    if os.path.exists(output_file):\n",
    "        with h5py.File(output_file, 'r') as h5_file:\n",
    "            # Everything matches\n",
    "            if 'version' in h5_file.attrs and h5_file.attrs['version'] == H5_VERSION:\n",
    "                # All good nothing to do\n",
    "                return\n",
    "\n",
    "    # Open the output files\n",
    "    with Timer('Processing input files'):\n",
    "        data1, labels1 = process_files(URL_ROOT + '/impulsive_broadband_simulation_random_5p', 1)\n",
    "        data2, labels2 = process_files(URL_ROOT + '/impulsive_broadband_simulation_random_10p', 1)\n",
    "        data3, labels3 = process_files(URL_ROOT + '/repetitive_rfi_timeseries', 1)\n",
    "        data4, labels4 = process_files(URL_ROOT + '/repetitive_rfi_random_timeseries', 1)\n",
    "        # data0, labels0 = process_files(URL_ROOT + '/impulsive_broadband_simulation_random_norfi', 0)\n",
    "\n",
    "    # Concatenate\n",
    "    with Timer('Concatenating data'):\n",
    "        labels = np.concatenate((labels1, labels2, labels3, labels4))\n",
    "        data = np.concatenate((data1, data2, data3, data4))\n",
    "\n",
    "    # Standardise and one hot\n",
    "    with Timer('Standardise & One hot'):\n",
    "        labels = one_hot(labels, NUMBER_OF_CLASSES)\n",
    "        # data = normalize(data)\n",
    "\n",
    "    with Timer('Saving to {0}'.format(output_file)):\n",
    "        if not exists(kwargs['data_path']):\n",
    "            makedirs(kwargs['data_path'])\n",
    "        with h5py.File(output_file, 'w') as h5_file:\n",
    "            h5_file.attrs['number_channels'] = NUMBER_CHANNELS\n",
    "            h5_file.attrs['number_classes'] = NUMBER_OF_CLASSES\n",
    "            h5_file.attrs['version'] = H5_VERSION\n",
    "\n",
    "            # If the data is standardised standardise the training data and then use the mean and std values to\n",
    "            # standardise the validation and training\n",
    "\n",
    "            data_group = h5_file.create_group('data')\n",
    "            data_group.attrs['length_data'] = len(data)\n",
    "            data_group.create_dataset('data_channel_0', data=data, compression='gzip')\n",
    "            data_group.create_dataset('labels', data=labels, compression='gzip')\n",
    "\n",
    "\n",
    "def normalize(all_data):\n",
    "    \"\"\" normalize data \"\"\"\n",
    "    min_value = np.min(all_data)\n",
    "    max_value = np.max(all_data)\n",
    "    return (all_data - min_value) / (max_value - min_value)\n",
    "\n",
    "\n",
    "def standardize(all_data):\n",
    "    \"\"\" Standardize data \"\"\"\n",
    "    return (all_data - np.mean(all_data)) / np.std(all_data)\n",
    "\n",
    "\n",
    "def one_hot(labels, number_class):\n",
    "    \"\"\" One-hot encoding \"\"\"\n",
    "    expansion = np.eye(number_class)\n",
    "    y = expansion[:, labels].T\n",
    "    assert y.shape[1] == number_class, \"Wrong number of labels!\"\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are helpers to show how long things take and to plot histograms in ASCII (for when you are working on Magnus or Athena and want to see how things are going)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Timer(object):\n",
    "    def __init__(self, name=None):\n",
    "        self.name = '' if name is None else name\n",
    "        self.timer = default_timer\n",
    "\n",
    "    def __enter__(self):\n",
    "        LOGGER.info('{}, Starting timer'.format(self.name))\n",
    "        self.start = self.timer()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        end = self.timer()\n",
    "        self.elapsed_secs = end - self.start\n",
    "        self.elapsed = self.elapsed_secs\n",
    "        LOGGER.info('{}, Elapsed time: {}'.format(self.name, human_time(self.elapsed)))\n",
    "\n",
    "\n",
    "class Histogram(object):\n",
    "    def __init__(self, data, bins=10, title=None, number_range=None, histogram_type='bars'):\n",
    "        self.bins = bins\n",
    "        self.title = title\n",
    "        self.type = histogram_type\n",
    "        self.histogram = np.histogram(np.array(data), bins=self.bins, range=number_range)\n",
    "        if histogram_type == 'numbers':\n",
    "            total = len(data)\n",
    "            self.percentages = [bin_value * 100.0 / total for bin_value in self.histogram[0]]\n",
    "\n",
    "    def horizontal(self, height=4, character='|'):\n",
    "        if self.title is not None:\n",
    "            his = \"{0}\\n\\n\".format(self.title)\n",
    "        else:\n",
    "            his = \"\"\n",
    "\n",
    "        if self.type == 'bars':\n",
    "            bars = self.histogram[0] / float(max(self.histogram[0])) * height\n",
    "            for reversed_height in reversed(range(1, height+1)):\n",
    "                if reversed_height == height:\n",
    "                    line = '{0} '.format(max(self.histogram[0]))\n",
    "                else:\n",
    "                    line = ' '*(len(str(max(self.histogram[0]))) + 1)\n",
    "                for c in bars:\n",
    "                    if c >= math.ceil(reversed_height):\n",
    "                        line += character\n",
    "                    else:\n",
    "                        line += ' '\n",
    "                line += '\\n'\n",
    "                his += line\n",
    "            his += '{0:.2f}'.format(self.histogram[1][0]) + ' ' * self.bins + '{0:.2f}'.format(self.histogram[1][-1]) + '\\n'\n",
    "        else:\n",
    "            his += ' ' * 4\n",
    "            his += ''.join(['| {0:^8.2f}%'.format(n) for n in self.percentages])\n",
    "            his += '|\\n'\n",
    "            his += ' ' * 4\n",
    "            his += ''.join(['| {0:^8} '.format(n) for n in self.histogram[0]])\n",
    "            his += '|\\n'\n",
    "            his += ' ' * 4\n",
    "            his += '|----------'*len(self.histogram[0])\n",
    "            his += '|\\n'\n",
    "            his += ''.join(['| {0:^8.2f} '.format(n) for n in self.histogram[1]])\n",
    "            his += '|\\n'\n",
    "        return his\n",
    "\n",
    "    def vertical(self, height=20, character='|'):\n",
    "        if self.title is not None:\n",
    "            his = \"{0}\\n\\n\".format(self.title)\n",
    "        else:\n",
    "            his = \"\"\n",
    "\n",
    "        if self.type == 'bars':\n",
    "            xl = ['{0:.2f}'.format(n) for n in self.histogram[1]]\n",
    "            lxl = [len(l) for l in xl]\n",
    "            bars = self.histogram[0] / float(max(self.histogram[0])) * height\n",
    "            bars = np.rint(bars).astype('uint32')\n",
    "            his += ' '*(max(bars)+2+max(lxl))+'{0}\\n'.format(max(self.histogram[0]))\n",
    "            for i, c in enumerate(bars):\n",
    "                line = xl[i] + ' '*(max(lxl)-lxl[i])+': ' + character*c+'\\n'\n",
    "                his += line\n",
    "        else:\n",
    "            for item1, item2, item3 in zip(self.histogram[0], self.histogram[1], self.percentages):\n",
    "                line = '{0:>6.2f} | {1:>5} | {2:>6.2f}%\\n'.format(item2, item1, item3)\n",
    "                his += line\n",
    "        return his\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the actual neural network.\n",
    "\n",
    "For this example I'm using a simple Deep Neural Network with various statistical measures augmenting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GmrtLinear(nn.Module):\n",
    "    def __init__(self, keep_probability, sequence_length):\n",
    "        super(GmrtLinear, self).__init__()\n",
    "        self.keep_probability = keep_probability\n",
    "        self.input_layer_length = 6 + (sequence_length * 7)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_layer_length, HIDDEN_LAYERS).double()\n",
    "        self.fc2 = nn.Linear(HIDDEN_LAYERS + self.input_layer_length, HIDDEN_LAYERS).double()\n",
    "        self.fc3 = nn.Linear(HIDDEN_LAYERS, HIDDEN_LAYERS).double()\n",
    "        self.fc4 = nn.Linear(HIDDEN_LAYERS, HIDDEN_LAYERS).double()\n",
    "        self.fc5 = nn.Linear(HIDDEN_LAYERS, HIDDEN_LAYERS).double()\n",
    "        self.fc6 = nn.Linear(HIDDEN_LAYERS, NUMBER_OF_CLASSES).double()\n",
    "\n",
    "    def forward(self, input_data_values):\n",
    "        x = functional.leaky_relu(self.fc1(input_data_values))\n",
    "        x = functional.leaky_relu(self.fc2(torch.cat((x, input_data_values), dim=1)))\n",
    "        x = functional.dropout(x, p=self.keep_probability, training=self.training)\n",
    "        x = functional.leaky_relu(self.fc3(x))\n",
    "        x = functional.leaky_relu(self.fc4(x))\n",
    "        x = functional.dropout(x, p=self.keep_probability, training=self.training)\n",
    "        x = functional.leaky_relu(self.fc5(x))\n",
    "        x = functional.leaky_relu(self.fc6(x))\n",
    "\n",
    "        x = functional.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section trains and tests the network. The code is written to run on GPUs and using the HOGWILD algorithm on multiple processes.\n",
    "\n",
    "At the end of each validation run the code prints a series of histograms showing how well the system did on the test data that the system has not seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, rfi_data, rank=0, **kwargs):\n",
    "    # This is needed to \"trick\" numpy into using different seeds for different processes\n",
    "    if kwargs['seed'] is not None:\n",
    "        np.random.seed(kwargs['seed'] + rank)\n",
    "    else:\n",
    "        np.random.seed()\n",
    "\n",
    "    train_loader = data.DataLoader(\n",
    "        rfi_data.get_rfi_dataset('training', rank=rank, short_run_size=kwargs['short_run']),\n",
    "        batch_size=kwargs['batch_size'],\n",
    "        num_workers=3,\n",
    "        pin_memory=kwargs['using_gpu'],\n",
    "    )\n",
    "    test_loader = data.DataLoader(\n",
    "        rfi_data.get_rfi_dataset('validation', rank=rank, short_run_size=kwargs['short_run']),\n",
    "        batch_size=kwargs['batch_size'],\n",
    "        num_workers=3,\n",
    "        pin_memory=kwargs['using_gpu'],\n",
    "    )\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=kwargs['learning_rate'], momentum=kwargs['momentum'])\n",
    "    for epoch in range(1, kwargs['epochs'] + 1):\n",
    "        # Adjust the learning rate\n",
    "        adjust_learning_rate(optimizer, epoch, kwargs['learning_rate_decay'], kwargs['start_learning_rate_decay'], kwargs['learning_rate'])\n",
    "        train_epoch(epoch, model, train_loader, optimizer, kwargs['log_interval'])\n",
    "        test_epoch(model, test_loader, kwargs['log_interval'])\n",
    "\n",
    "\n",
    "def train_epoch(epoch, model, data_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (x_data_raw, target) in enumerate(data_loader):\n",
    "        x_data_raw = Variable(x_data_raw)\n",
    "        target = Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_data_raw)\n",
    "        if type(output.data) == torch.cuda.DoubleTensor:\n",
    "            output = output.cpu()\n",
    "        loss = functional.binary_cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0 and batch_idx > 1:\n",
    "            LOGGER.info('Train Epoch: {} [{}/{} ({:.0f}%)], Loss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(x_data_raw),\n",
    "                len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader),\n",
    "                loss.data[0])\n",
    "            )\n",
    "\n",
    "\n",
    "def build_histogram(output, target_column, histogram_data):\n",
    "    for values, column in zip(output.data.numpy(), target_column.numpy()):\n",
    "        histogram_data['all'].append(values[column])\n",
    "        histogram_data[column].append(values[column])\n",
    "\n",
    "\n",
    "def test_epoch(model, data_loader, log_interval):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    histogram_data = {key: [] for key in range(NUMBER_OF_CLASSES)}\n",
    "    histogram_data['all'] = []\n",
    "    for batch_index, (x_data_raw, target) in enumerate(data_loader):\n",
    "        x_data_raw = Variable(x_data_raw, volatile=True)\n",
    "        target = Variable(target)\n",
    "        output = model(x_data_raw)\n",
    "        if type(output.data) == torch.cuda.DoubleTensor:\n",
    "            output = output.cpu()\n",
    "        test_loss += functional.binary_cross_entropy(output, target).data[0]\n",
    "        pred = output.data.max(1)[1]\n",
    "        target_column = target.data.max(1)[1]\n",
    "        correct += pred.eq(target_column).sum()\n",
    "        build_histogram(output, target_column, histogram_data)\n",
    "\n",
    "        if batch_index % log_interval == 0 and batch_index > 1:\n",
    "            LOGGER.info('Test iteration: {}, Correct count: {}'.format(batch_index, correct))\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    LOGGER.info('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss,\n",
    "        correct,\n",
    "        len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset))\n",
    "    )\n",
    "    for key, value in histogram_data.items():\n",
    "        histogram = Histogram(\n",
    "            value,\n",
    "            title='Percentage of Correctly Predicted: {}'.format(key),\n",
    "            bins=10,\n",
    "            number_range=(0.0, 1.0),\n",
    "            histogram_type='numbers'\n",
    "        )\n",
    "        LOGGER.info(histogram.horizontal())\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, learning_rate_decay, start_learning_rate_decay, learning_rate):\n",
    "    \"\"\" Sets the learning rate to the initial LR decayed  \"\"\"\n",
    "    lr_decay = learning_rate_decay ** max(epoch + 1 - start_learning_rate_decay, 0.0)\n",
    "    new_learning_rate = learning_rate * lr_decay\n",
    "    LOGGER.info('New learning rate: {}'.format(new_learning_rate))\n",
    "    for param_group in optimizer.state_dict()['param_groups']:\n",
    "        param_group['lr'] = new_learning_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section runs the programs. \n",
    "\n",
    "All you need to do to modify the parameters is \"tweak\" the defaults. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s:%(process)d:%(levelname)s:%(name)s:%(message)s')\n",
    "    parser = argparse.ArgumentParser(description='GMRT DNN Training')\n",
    "    parser.add_argument('--batch_size', type=int, default=20000, metavar='N', help='input batch size for training (default: 20000)')\n",
    "    parser.add_argument('--epochs', type=int, default=5, metavar='N', help='number of epochs to train (default: 5)')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01, metavar='LR', help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, metavar='M', help='SGD momentum (default: 0.5)')\n",
    "    parser.add_argument('--keep_probability', type=float, default=0.6, metavar='K', help='Dropout keep probability (default: 0.6)')\n",
    "    parser.add_argument('--log_interval', type=int, default=10, metavar='N', help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--num_processes', type=int, default=2, metavar='N', help='how many training processes to use (default: 4)')\n",
    "    parser.add_argument('--use_gpu', action='store_true', default=False, help='use the GPU if it is available')\n",
    "    parser.add_argument('--data_path', default='/tmp', help='the path to the data file')\n",
    "    parser.add_argument('--data_file', default='data.h5', help='the name of the data file')\n",
    "    parser.add_argument('--sequence_length', type=int, default=10, help='how many elements in a sequence')\n",
    "    parser.add_argument('--validation_percentage', type=int, default=10, help='amount of data used for validation')\n",
    "    parser.add_argument('--training_percentage', type=int, default=80, help='amount of data used for training')\n",
    "    parser.add_argument('--seed', type=int, default=None, metavar='S', help='random seed (default: 1)')\n",
    "    parser.add_argument('--learning_rate_decay', type=float, default=0.8, metavar='LRD', help='the initial learning rate decay rate')\n",
    "    parser.add_argument('--start_learning_rate_decay', type=int, default=5, help='the epoch to start applying the LRD')\n",
    "    parser.add_argument('--short_run', type=int, default=None, help='use a short run of the test data')\n",
    "    parser.add_argument('--save', type=str,  default=None, help='path to save the final model')\n",
    "    parser.add_argument('--dataset', type=int,  default=1, help='the dataset you wish to run')\n",
    "\n",
    "    kwargs = vars(parser.parse_args())\n",
    "    LOGGER.debug(kwargs)\n",
    "\n",
    "    # If the have specified a seed get a random\n",
    "    if kwargs['seed'] is not None:\n",
    "        np.random.seed(kwargs['seed'])\n",
    "    else:\n",
    "        np.random.seed()\n",
    "\n",
    "    if kwargs['use_gpu'] and torch.cuda.is_available():\n",
    "        LOGGER.info('Using cuda devices: {}'.format(torch.cuda.device_count()))\n",
    "        kwargs['cuda_device_count'] = torch.cuda.device_count()\n",
    "        kwargs['using_gpu'] = True\n",
    "    else:\n",
    "        LOGGER.info('Using CPU')\n",
    "        kwargs['cuda_device_count'] = 0\n",
    "        kwargs['using_gpu'] = False\n",
    "\n",
    "    # Do this first so all the data is built before we go parallel and get race conditions\n",
    "    with Timer('Checking/Building data file'):\n",
    "        build_data(**kwargs)\n",
    "\n",
    "    rfi_data = RfiData(**kwargs)\n",
    "\n",
    "    if kwargs['using_gpu']:\n",
    "        # The DataParallel will distribute the model to all the available GPUs\n",
    "        model = nn.DataParallel(GmrtLinear(kwargs['keep_probability'], kwargs['sequence_length'])).cuda()\n",
    "\n",
    "        # Train\n",
    "        train(model, rfi_data, **kwargs)\n",
    "\n",
    "    else:\n",
    "        # This uses the HOGWILD! approach to lock free SGD\n",
    "        model = GmrtLinear(kwargs['keep_probability'], kwargs['sequence_length'])\n",
    "        model.share_memory()  # gradients are allocated lazily, so they are not shared here\n",
    "\n",
    "        processes = []\n",
    "        for rank in range(kwargs['num_processes']):\n",
    "            p = mp.Process(target=train, args=(model, rfi_data, rank), kwargs=kwargs)\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "\n",
    "    with Timer('Reading final test data'):\n",
    "        test_loader = data.DataLoader(\n",
    "            rfi_data.get_rfi_dataset('test', short_run_size=kwargs['short_run']),\n",
    "            batch_size=kwargs['batch_size'],\n",
    "            num_workers=3,\n",
    "            pin_memory=kwargs['using_gpu'],\n",
    "        )\n",
    "    with Timer('Final test'):\n",
    "        test_epoch(model, test_loader, kwargs['log_interval'])\n",
    "\n",
    "    if kwargs['save'] is not None:\n",
    "        with Timer('Saving model'):\n",
    "            with open(kwargs['save'], 'wb') as save_file:\n",
    "                torch.save(model.state_dict(), save_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['program_name']\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 (rfi_ml)",
   "language": "python",
   "name": "rfi_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
